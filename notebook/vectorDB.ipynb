{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "import json\n",
    "import os\n",
    "from langchain.schema import Document\n",
    "\n",
    "txt_dir = \"../data/txt\"  # Directory for text files\n",
    "json_dir = \"../data/gpt-json\"  # Directory for JSON files\n",
    "\n",
    "# Load text files\n",
    "def load_txt_docs(txt_dir):\n",
    "    loader = DirectoryLoader(txt_dir, glob=\"**/*.txt\", loader_cls=TextLoader, loader_kwargs={\"encoding\": \"utf-8\"})\n",
    "    return loader.load()\n",
    "\n",
    "def load_json_docs(json_dir):\n",
    "    docs = []\n",
    "    for filename in os.listdir(json_dir):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(json_dir, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                json_data = json.load(f)\n",
    "\n",
    "            for entry in json_data:\n",
    "                query = entry.get(\"Query\", \"\").strip()\n",
    "                solution = entry.get(\"Solution\", \"\").strip()\n",
    "                source = entry.get(\"source\", filename)  # Use filename if \"source\" is missing\n",
    "                keywords = entry.get(\"keywords\", [])  # Extract keywords list\n",
    "                \n",
    "                # Convert list of keywords to a comma-separated string (fix for ChromaDB)\n",
    "                keywords_str = \", \".join(keywords) if isinstance(keywords, list) else \"\"\n",
    "\n",
    "                # Log missing keywords\n",
    "                if not keywords:\n",
    "                    print(f\"⚠️ Warning: No keywords found for query: \\\"{query}\\\" in {source}\")\n",
    "\n",
    "                # Combine query & solution into one document\n",
    "                content = f\"Query: {query}\\nSolution: {solution}\"\n",
    "\n",
    "                # Append document with metadata including keywords as a string\n",
    "                docs.append(Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\"source\": source, \"keywords\": keywords_str}  # Fix applied\n",
    "                ))\n",
    "    return docs\n",
    "\n",
    "\n",
    "# Load both text and JSON files\n",
    "txt_docs = load_txt_docs(txt_dir)\n",
    "json_docs = load_json_docs(json_dir)\n",
    "\n",
    "# Combine all documents\n",
    "all_docs = txt_docs + json_docs\n",
    "\n",
    "# Print total count\n",
    "print(f\"Total documents loaded: {len(all_docs)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all loaded documents\n",
    "for i, doc in enumerate(all_docs, 1):\n",
    "    print(f\"Document {i} (Source: {doc.metadata['source']}):\\nKeywords: {doc.metadata.get('keywords', [])}\\n{doc.page_content}\\n{'-'*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def split_documents_by_qna(docs, max_chunk_size=2048):\n",
    "    \"\"\"\n",
    "    Splits documents into smaller chunks while ensuring each 'Query & Solution' pair remains intact.\n",
    "    Dynamically adjusts chunking to prevent sentence truncation.\n",
    "    \"\"\"\n",
    "    split_chunks = []\n",
    "\n",
    "    for doc in docs:\n",
    "        content = doc.page_content.strip()\n",
    "        metadata = doc.metadata.copy()  # Copy metadata to prevent mutation issues\n",
    "\n",
    "        # Ensure \"Query: \" formatting correctly without duplication\n",
    "        if not content.startswith(\"Query: \"):\n",
    "            content = \"Query: \" + content  # Add only if missing\n",
    "\n",
    "        # Split into individual Q&A pairs using '\\nQuery: ' as the separator\n",
    "        qna_pairs = content.split(\"\\nQuery: \")  \n",
    "        qna_pairs = [pair.strip() for pair in qna_pairs if pair.strip()]  # Ensure proper formatting\n",
    "\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for qna in qna_pairs:\n",
    "            # Make sure each chunk starts with \"Query: \" exactly once\n",
    "            if not qna.startswith(\"Query: \"):\n",
    "                qna = \"Query: \" + qna  # Fix edge cases where it's missing after splitting\n",
    "\n",
    "            if len(current_chunk) + len(qna) > max_chunk_size:\n",
    "                # Save the current chunk before starting a new one\n",
    "                split_chunks.append(Document(page_content=current_chunk.strip(), metadata=metadata))\n",
    "                current_chunk = \"\"\n",
    "\n",
    "            # If a single Q&A is too big, store it separately\n",
    "            if len(qna) > max_chunk_size:\n",
    "                split_chunks.append(Document(page_content=qna.strip(), metadata=metadata))\n",
    "            else:\n",
    "                current_chunk += qna + \"\\n\\n\"  # Append Q&A pair with spacing\n",
    "\n",
    "        # Add last chunk if not empty\n",
    "        if current_chunk.strip():\n",
    "            split_chunks.append(Document(page_content=current_chunk.strip(), metadata=metadata))\n",
    "\n",
    "    return split_chunks\n",
    "\n",
    "# Apply structured splitting\n",
    "docs = split_documents_by_qna(all_docs)\n",
    "\n",
    "print(f\"✅ Total chunks after structured splitting: {len(docs)}\\n\")\n",
    "print(f\"Example Chunk:\\n{docs[1].page_content}\\n\")\n",
    "print(f\"Example Last Chunk:\\n{docs[-1].page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "#."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating vector db\n",
    "from langchain_chroma import Chroma\n",
    "db = Chroma.from_documents(docs, embeddings)\n",
    "#."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what will happen if i fail to fill itr\"\n",
    "matching_docs = db.similarity_search(query)\n",
    "#."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_docs\n",
    "#."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "#."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever\n",
    "#."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"../model/taxadvisordb_v1-0\"\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "#."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "#."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_docs = new_db.similarity_search_with_score(query,k=3)\n",
    "\n",
    "matching_docs\n",
    "\n",
    "#."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what will happen if i fail to fill itr\"\n",
    "similar_docs = new_db.similarity_search(query, k=1)\n",
    "\n",
    "for doc in similar_docs:\n",
    "    print(doc.page_content)\n",
    "\n",
    "#."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
